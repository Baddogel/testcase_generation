{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Cove\\Python Projects\\testcase_generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/gpt2-testcase'\n",
    "DATASET_PATH = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предобработанного датасета\n",
    "dataset = load_from_disk(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение датасета на тренировочный и проверочный\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора и модели\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Настройка токенизатора (у GPT-2 нет pad_token по умолчанию)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 109\n"
     ]
    }
   ],
   "source": [
    "# Определение максимальной длины последовательности в токенах\n",
    "def get_max_length(dataset, tokenizer):\n",
    "    max_len = 0\n",
    "    for example in dataset:\n",
    "        scenario = example['test_scenario']\n",
    "        steps = example['test_steps']\n",
    "        text = f\"test_scenario:{scenario}\\ntest_steps:{steps}\"\n",
    "        tokens = tokenizer(text, return_tensors='pt', truncation=False, padding=False)\n",
    "        current_len = tokens['input_ids'].shape[1]\n",
    "        if current_len > max_len:\n",
    "            max_len = current_len\n",
    "    return min(max_len, model.config.n_positions)  # Ограничение максимальной длины модели\n",
    "\n",
    "max_length = get_max_length(train_dataset, tokenizer)\n",
    "print(f\"Max sequence length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предобработки данных\n",
    "def preprocess_function(examples):\n",
    "    scenarios = examples['test_scenario']\n",
    "    steps = examples['test_steps']\n",
    "    texts = [f\"test_scenario:\\n{scenario}\\ntest_steps:\\n{step}\" for scenario, step in zip(scenarios, steps)]\n",
    "    model_inputs = tokenizer(texts, max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    # Создание labels с маскированием сценария\n",
    "    labels = []\n",
    "    for i in range(len(texts)):\n",
    "        # Токенизация сценария для определения его длины\n",
    "        scenario_tokens = tokenizer(scenarios[i], add_special_tokens=False)\n",
    "        scenario_length = len(scenario_tokens['input_ids'])\n",
    "        # Добавляем 1 для токена новой строки\n",
    "        sep_token = '\\n'\n",
    "        sep_token_ids = tokenizer(sep_token, add_special_tokens=False)['input_ids']\n",
    "        total_scenario_length = scenario_length + len(sep_token_ids)\n",
    "        \n",
    "        # Маскируем сценарий и разделитель\n",
    "        label = [-100] * total_scenario_length + model_inputs['input_ids'][i][total_scenario_length:]\n",
    "        labels.append(label)\n",
    "    \n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3624/3624 [00:01<00:00, 2135.92 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 2026.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Применение предобработки\n",
    "train_dataset_p = train_dataset.map(preprocess_function, batched=True, remove_columns=['test_scenario', 'test_steps'])\n",
    "val_dataset_p = val_dataset.map(preprocess_function, batched=True, remove_columns=['test_scenario', 'test_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для коллекции батчей\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Создание DataLoader'ов с collate_fn\n",
    "train_dataloader = DataLoader(train_dataset_p, batch_size=6, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset_p, batch_size=6, collate_fn=collate_fn)\n",
    "\n",
    "# Настройка устройства и модели\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смешанная точность для ускорения обучения\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12080 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "  5%|▌         | 606/12080 [00:35<1:23:07,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20, Validation Loss: 0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1211/12080 [01:08<59:32,  3.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20, Validation Loss: 0.5359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1814/12080 [01:40<56:38,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20, Validation Loss: 0.4870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2419/12080 [02:13<53:08,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20, Validation Loss: 0.4470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3021/12080 [02:46<49:56,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20, Validation Loss: 0.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3627/12080 [03:19<46:52,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20, Validation Loss: 0.4011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 4229/12080 [03:52<43:04,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20, Validation Loss: 0.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4834/12080 [04:25<39:49,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20, Validation Loss: 0.3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5440/12080 [04:57<27:16,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20, Validation Loss: 0.3597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6041/12080 [05:30<33:16,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20, Validation Loss: 0.3518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 6647/12080 [06:03<24:15,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20, Validation Loss: 0.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 7249/12080 [06:35<26:35,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20, Validation Loss: 0.3418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 7855/12080 [07:08<23:17,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20, Validation Loss: 0.3407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 8459/12080 [07:41<16:11,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20, Validation Loss: 0.3364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9063/12080 [08:14<16:36,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20, Validation Loss: 0.3322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 9666/12080 [08:46<13:18,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20, Validation Loss: 0.3329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 10269/12080 [09:19<09:57,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20, Validation Loss: 0.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 10875/12080 [09:52<06:38,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20, Validation Loss: 0.3303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 11479/12080 [10:25<03:19,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20, Validation Loss: 0.3312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 12079/12080 [10:54<00:00, 20.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20, Validation Loss: 0.3307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/gpt2-testcase\\\\tokenizer_config.json',\n",
       " 'models/gpt2-testcase\\\\special_tokens_map.json',\n",
       " 'models/gpt2-testcase\\\\vocab.json',\n",
       " 'models/gpt2-testcase\\\\merges.txt',\n",
       " 'models/gpt2-testcase\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12080/12080 [11:06<00:00, 20.27it/s]"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Валидация после эпохи\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "# Сохранение модели и токенизатора\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Загрузка модели\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "# Перемещение модели на GPU или CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_scenario:\n",
      "Verify that a user can successfully subscribe to the sports and fitness equipment website.\n",
      "test_steps:\n",
      "1. Navigate to the subscription page.\n",
      "2. Select a subscription plan.\n",
      "3. Enter valid payment details.\n",
      "4. Click on the \"Subscribe\" button.\n"
     ]
    }
   ],
   "source": [
    "# Проверка генерации\n",
    "model.eval()\n",
    "# Берем случайный пример из валидационного датасета\n",
    "random_sample = val_dataset.shuffle().select([0])['test_scenario'][0]\n",
    "# Токенизация с настройкой максимальной длины контекста\n",
    "inputs = tokenizer(\n",
    "    f\"test_scenario:\\n{random_sample}\\ntest_steps:\\n\", \n",
    "    return_tensors='pt',\n",
    "    max_length=model.config.n_positions,  # Максимальная длина для модели\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,  # Увеличенная максимальная длина\n",
    "        min_length=50,   # Минимальная длина ответа\n",
    "        num_beams=5,     # Лучший поиск для качества\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=False,  # Отключаем преждевременную остановку\n",
    "        do_sample=True,        # Включаем сэмплирование\n",
    "        temperature=0.7,       # Температура для разнообразия\n",
    "        top_p=0.9,             # Ядерная выборка\n",
    "        top_k=50               # Ограничение топ-k токенов\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
