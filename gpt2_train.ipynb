{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/distilgpt2-finetuned'\n",
    "DATASET_PATH = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предобработанного датасета\n",
    "dataset = load_from_disk(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение датасета на тренировочный и проверочный\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора и модели\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Настройка токенизатора (у GPT-2 нет pad_token по умолчанию)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 326\n"
     ]
    }
   ],
   "source": [
    "# Определение максимальной длины последовательности\n",
    "def get_max_length(dataset, tokenizer):\n",
    "    max_len = 0\n",
    "    for example in dataset:\n",
    "        scenario = example['test_scenario']\n",
    "        steps = example['test_steps']\n",
    "        text = f\"test_scenario:{scenario}\\ntest_steps:{steps}\"\n",
    "        tokens = tokenizer(text, return_tensors='pt', truncation=False, padding=False)\n",
    "        current_len = tokens['input_ids'].shape[1]\n",
    "        if current_len > max_len:\n",
    "            max_len = current_len\n",
    "    return min(max_len, model.config.n_positions)  # Ограничение максимальной длины модели\n",
    "\n",
    "max_length = get_max_length(train_dataset, tokenizer)\n",
    "print(f\"Max sequence length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предобработки данных\n",
    "def preprocess_function(examples):\n",
    "    scenarios = examples['test_scenario']\n",
    "    steps = examples['test_steps']\n",
    "    texts = [f\"test_scenario:\\n{scenario}\\ntest_steps:\\n{step}\" for scenario, step in zip(scenarios, steps)]\n",
    "    model_inputs = tokenizer(texts, max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    # Создание labels с маскированием сценария\n",
    "    labels = []\n",
    "    for i in range(len(texts)):\n",
    "        # Токенизация сценария для определения его длины\n",
    "        scenario_tokens = tokenizer(scenarios[i], add_special_tokens=False)\n",
    "        scenario_length = len(scenario_tokens['input_ids'])\n",
    "        # Добавляем 1 для токена новой строки\n",
    "        sep_token = '\\n'\n",
    "        sep_token_ids = tokenizer(sep_token, add_special_tokens=False)['input_ids']\n",
    "        total_scenario_length = scenario_length + len(sep_token_ids)\n",
    "        \n",
    "        # Маскируем сценарий и разделитель\n",
    "        label = [-100] * total_scenario_length + model_inputs['input_ids'][i][total_scenario_length:]\n",
    "        labels.append(label)\n",
    "    \n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3738/3738 [00:02<00:00, 1774.95 examples/s]\n",
      "Map: 100%|██████████| 935/935 [00:00<00:00, 1785.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Применение предобработки\n",
    "train_dataset_p = train_dataset.map(preprocess_function, batched=True, remove_columns=['test_scenario', 'test_steps'])\n",
    "val_dataset_p = val_dataset.map(preprocess_function, batched=True, remove_columns=['test_scenario', 'test_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для коллекции батчей\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Создание DataLoader'ов с collate_fn\n",
    "train_dataloader = DataLoader(train_dataset_p, batch_size=6, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset_p, batch_size=6, collate_fn=collate_fn)\n",
    "\n",
    "# Настройка устройства и модели\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смешанная точность для ускорения обучения\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12460 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "c:\\Cove\\Python Projects\\testcase_gpt2\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "  5%|▌         | 624/12460 [01:17<8:56:15,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20, Validation Loss: 0.2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1247/12460 [02:32<8:25:31,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20, Validation Loss: 0.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1870/12460 [03:48<7:56:49,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20, Validation Loss: 0.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2493/12460 [05:03<7:29:04,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20, Validation Loss: 0.1509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3116/12460 [06:18<7:00:55,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20, Validation Loss: 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3739/12460 [07:32<6:32:37,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20, Validation Loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 4362/12460 [08:47<6:05:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20, Validation Loss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4985/12460 [10:02<5:36:51,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20, Validation Loss: 0.1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5608/12460 [11:17<5:09:43,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20, Validation Loss: 0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6231/12460 [12:32<4:42:08,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20, Validation Loss: 0.1159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 6854/12460 [13:47<4:13:56,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20, Validation Loss: 0.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 7477/12460 [15:02<3:44:50,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20, Validation Loss: 0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 8100/12460 [16:17<3:17:29,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20, Validation Loss: 0.1096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 8723/12460 [17:32<2:49:02,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20, Validation Loss: 0.1075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9346/12460 [18:47<2:21:03,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20, Validation Loss: 0.1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 9969/12460 [20:02<1:52:52,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20, Validation Loss: 0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 10592/12460 [21:17<1:24:36,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20, Validation Loss: 0.1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 11215/12460 [22:32<56:25,  2.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20, Validation Loss: 0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 11838/12460 [23:48<28:15,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20, Validation Loss: 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12460/12460 [24:54<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20, Validation Loss: 0.1042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilgpt2-finetuned\\\\tokenizer_config.json',\n",
       " './distilgpt2-finetuned\\\\special_tokens_map.json',\n",
       " './distilgpt2-finetuned\\\\vocab.json',\n",
       " './distilgpt2-finetuned\\\\merges.txt',\n",
       " './distilgpt2-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение модели\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Валидация после эпохи\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "# Сохранение модели и токенизатора\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Загрузка модели\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "# Перемещение модели на GPU или CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_scenario:\n",
      "Test the functionality for recording the live class session.\n",
      "test_steps:\n",
      "1. Access the live classes section.\n",
      "2. Click on a specific recording feature.\n",
      "3. Record the live session using the recorded feature.\n"
     ]
    }
   ],
   "source": [
    "# Проверка генерации\n",
    "model.eval()\n",
    "# Берем случайный пример из валидационного датасета\n",
    "random_sample = val_dataset.shuffle().select([0])['test_scenario'][0]\n",
    "# Токенизация с настройкой максимальной длины контекста\n",
    "inputs = tokenizer(\n",
    "    f\"test_scenario:\\n{random_sample}\\ntest_steps:\\n\", \n",
    "    return_tensors='pt',\n",
    "    max_length=model.config.n_positions,  # Максимальная длина для модели\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,  # Увеличенная максимальная длина\n",
    "        min_length=50,   # Минимальная длина ответа\n",
    "        num_beams=5,     # Лучший поиск для качества\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=False,  # Отключаем преждевременную остановку\n",
    "        do_sample=True,        # Включаем сэмплирование\n",
    "        temperature=0.7,       # Температура для разнообразия\n",
    "        top_p=0.9,             # Ядерная выборка\n",
    "        top_k=50               # Ограничение топ-k токенов\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
