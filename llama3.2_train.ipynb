{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Cove\\Python Projects\\testcase_generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Константы\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "MODEL_PATH = \"./models/llama3.2-testcase\"\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_from_disk(\"dataset\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и настройка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Загрузка модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train_dataset sequence length: 98\n",
      "Max test_dataset sequence length: 92\n"
     ]
    }
   ],
   "source": [
    "# Определение максимальной длины последовательности в токенах\n",
    "def get_max_length(dataset, tokenizer):\n",
    "    max_len = 0\n",
    "    for example in dataset:\n",
    "        scenario = example['test_scenario']\n",
    "        steps = example['test_steps']\n",
    "        text = f\"test_scenario:{scenario}\\ntest_steps:{steps}\"\n",
    "        tokens = tokenizer(text, return_tensors='pt', truncation=False, padding=False)\n",
    "        current_len = tokens['input_ids'].shape[1]\n",
    "        if current_len > max_len:\n",
    "            max_len = current_len\n",
    "    return max_len\n",
    "\n",
    "max_length_train = get_max_length(train_dataset, tokenizer)\n",
    "max_length_test = get_max_length(test_dataset, tokenizer)\n",
    "print(f\"Max train_dataset sequence length: {max_length_train}\")\n",
    "print(f\"Max test_dataset sequence length: {max_length_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3624/3624 [00:00<00:00, 18211.15 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 18875.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Препроцессинг\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"test_scenario:\\n{scenario}\\ntest_steps:\\n{step}<|end_of_text|>\" \n",
    "             for scenario, step in zip(examples['test_scenario'], examples['test_steps'])]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=98, truncation=True, padding=True)\n",
    "    return model_inputs\n",
    "\n",
    "# Применяем предобработку\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_15740\\2881115515.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Настройка Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Тренировочные аргументы\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    log_level='info',\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01, # Регуляризация\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Инициализация тренера\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3,624\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2,260\n",
      "  Number of trainable parameters = 1,235,814,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='2260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/2260 1:37:05 < 2:27:01, 0.15 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.935100</td>\n",
       "      <td>1.377245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.172400</td>\n",
       "      <td>1.032737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.987400</td>\n",
       "      <td>0.946957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.907900</td>\n",
       "      <td>0.869639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.763200</td>\n",
       "      <td>0.827481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.781371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.624100</td>\n",
       "      <td>0.764081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.595600</td>\n",
       "      <td>0.726053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.687277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.710230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.388400</td>\n",
       "      <td>0.689415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.669518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.392200</td>\n",
       "      <td>0.636662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.682215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.680962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.663944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.648793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.654793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./models/llama3.2-testcase\\checkpoint-500\n",
      "Configuration saved in ./models/llama3.2-testcase\\checkpoint-500\\config.json\n",
      "Configuration saved in ./models/llama3.2-testcase\\checkpoint-500\\generation_config.json\n",
      "Model weights saved in ./models/llama3.2-testcase\\checkpoint-500\\model.safetensors\n",
      "tokenizer config file saved in ./models/llama3.2-testcase\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./models/llama3.2-testcase\\checkpoint-500\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 906\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in ./models/llama3.2-testcase\\config.json\n",
      "Configuration saved in ./models/llama3.2-testcase\\generation_config.json\n",
      "Model weights saved in ./models/llama3.2-testcase\\model.safetensors\n",
      "tokenizer config file saved in ./models/llama3.2-testcase\\tokenizer_config.json\n",
      "Special tokens file saved in ./models/llama3.2-testcase\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/llama3.2-testcase\\\\tokenizer_config.json',\n",
       " './models/llama3.2-testcase\\\\special_tokens_map.json',\n",
       " './models/llama3.2-testcase\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение модели\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение модели\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Вход: Verify that users can view suggested groups based on events they've attended.\n",
      "Ожидаемый результат: 1. Log in to the social networking app using valid credentials.\n",
      "2. Navigate to the account management section.\n",
      "3. Check for the \"Suggested Groups\" tab or section.\n",
      "4. Verify that the suggested groups are based on events the user has attended.\n",
      "Сгенерированный результат: test_scenario:\n",
      "Verify that users can view suggested groups based on events they've attended.\n",
      "test_steps:\n",
      "1. Log in to the social networking app using valid credentials.\n",
      "2. Navigate to the \"Events\" section.\n",
      "3. Click on an event that the user has attended.\n",
      "4. Check if suggested groups are displayed based on the attended event.\n",
      "================================================================================\n",
      "Пример 2:\n",
      "Вход: User tries to change username without entering any value.\n",
      "Ожидаемый результат: 1. Log in to the social networking app.\n",
      "2. Navigate to the account settings.\n",
      "3. Click on the \"Edit Profile\" option.\n",
      "4. Leave the username field blank.\n",
      "5. Save the changes.\n",
      "Сгенерированный результат: test_scenario:\n",
      "User tries to change username without entering any value.\n",
      "test_steps:\n",
      "1. Login to the social networking app using valid credentials.\n",
      "2. Navigate to the account settings.\n",
      "3. Click on the \"Edit Profile\" option.\n",
      "4. Leave the username field blank.\n",
      "5. Save the changes.\n",
      "================================================================================\n",
      "Пример 3:\n",
      "Вход: Test the system's handling of switching profiles when multiple profiles are open simultaneously.\n",
      "Ожидаемый результат: 1. Log in with Profile A and open Profile B in a new tab.\n",
      "2. Make changes in Profile B.\n",
      "3. Switch back to Profile A.\n",
      "Сгенерированный результат: test_scenario:\n",
      "Test the system's handling of switching profiles when multiple profiles are open simultaneously.\n",
      "test_steps:\n",
      "1. Log in with Profile A and open Profile B in a new tab.\n",
      "2. Make changes in Profile B.\n",
      "3. Save the changes.\n",
      "4. Switch back to Profile A.\n",
      "================================================================================\n",
      "Пример 4:\n",
      "Вход: Verify that the event management platform can scale server resources to accommodate an increase in event attendees.\n",
      "Ожидаемый результат: 1. Simulate a scenario with a sudden increase in event attendees.\n",
      "2. Monitor the platform's response to automatically scale server resources.\n",
      "Сгенерированный результат: test_scenario:\n",
      "Verify that the event management platform can scale server resources to accommodate an increase in event attendees.\n",
      "test_steps:\n",
      "1. Increase the number of events being managed by the platform.\n",
      "2. Monitor the platform's response by adjusting server resources.\n",
      "================================================================================\n",
      "Пример 5:\n",
      "Вход: Verify that recommendations are updated in real-time when a user searches for a specific genre.\n",
      "Ожидаемый результат: 1. User logs into the streaming platform.\n",
      "2. User searches for a specific genre (e.g., Action, Comedy).\n",
      "3. Check if the recommendation section is updated immediately based on the searched genre.\n",
      "Сгенерированный результат: test_scenario:\n",
      "Verify that recommendations are updated in real-time when a user searches for a specific genre.\n",
      "test_steps:\n",
      "1. Login to the streaming platform using valid credentials.\n",
      "2. Navigate to the content discovery section.\n",
      "3. Enter a broad genre keyword (e.g., \"Drama\") in the search bar.\n",
      "4. Check if the recommended content is updated immediately based on the searched genre.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Генерация примеров\n",
    "def generate_examples(model, tokenizer, dataset, num_examples=5):\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        sample = random.choice(dataset)\n",
    "        inputs = tokenizer(f\"test_scenario:\\n{sample[\"test_scenario\"]}\\ntest_steps:\\n\", return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=5,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.9,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        results.append({\n",
    "            \"Input\": sample[\"test_scenario\"],\n",
    "            \"Generated\": generated,\n",
    "            \"Expected\": sample[\"test_steps\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Проверка результатов\n",
    "examples = generate_examples(model, tokenizer, test_dataset)\n",
    "for i, ex in enumerate(examples, 1):\n",
    "    print(f\"Пример {i}:\")\n",
    "    print(f\"Вход: {ex['Input']}\")\n",
    "    print(f\"Ожидаемый результат: {ex['Expected']}\")\n",
    "    print(f\"Сгенерированный результат: {ex['Generated']}\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
