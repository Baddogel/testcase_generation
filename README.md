## Структура проекта

 - [dataset_preprocessing.ipynb](./dataset_preprocessing.ipynb) - предобработка датасета
 - [gpt2_train.ipynb](./gpt2_train.ipynb) - дообучение GPT-2
 - [t5_train.ipynb](./t5_train.ipynb) - дообучение T5
 - [bart_train.ipynb](./bart_train.ipynb) - дообучение BART
 - [llama3.2_train.ipynb](./llama3.2_train.ipynb) - дообучение Llama 3.2

Ссылка на предобработанный датасет:
https://drive.google.com/file/d/14cNUtYwYOkg6RJY4CgrhQWBINYezOpHW/view?usp=sharing  
Содержимое архива необходимо извлечь в корневую папку проекта

Ссылка на дообученную модель GPT-2:  
https://drive.google.com/file/d/1K4i2zcaN92NyuH7r_FNHr60iBS1bw-7B/view?usp=sharing  
Содержимое архива необходимо извлечь в папку models

Ссылка на дообученную модель T5:  
https://drive.google.com/file/d/19ZlfJKqzj1lA6RdFut7sooTa4_yACWPO/view?usp=sharing  
Содержимое архива необходимо извлечь в папку models

Ссылка на дообученную модель BART:  
https://drive.google.com/file/d/1GxmEvIsfL6nXtQXPbpb-YM24au7ZMwf1/view?usp=sharing  
Содержимое архива необходимо извлечь в папку models

Ссылка на дообученную модель Llama 3.2:  
https://drive.google.com/file/d/1Jxcms0fUlkm6Cl5v1jjmriirS279GoQt/view?usp=sharing  
Содержимое архива необходимо извлечь в папку models